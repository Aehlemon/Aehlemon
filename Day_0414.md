## 👀 서포트 벡터머신 (Support Vector Machine)

- 개념:

  ***서로 다른 분류에 속한 데이터 간에 간격(마진, Margin)이 최대가 되는 선(또는 초평면, hyperplane)을***

  ***찾아서 이를 기준으로 데이터를 분류하는 모델***

  

  - Support vector (지지 벡터):  초평면에 가장 가까운 훈련 샘플
  -  Margin: 초평면과 가장 가까운 훈련 샘플(서포트 벡터) 사이 거리

  - 두 범주 또는 여러 class 간의 데이터를 분류하는 데 성능이 뛰어남

  - Machine Learning의 한 종류로 지도학습 모델이며, `텍스트 분류, 패턴인식, 분류 및 회귀`에 주로 사용되는데 분류는 이진 분류를 위한 기법 중 하나로서 N 차원의 공간을 N-1 차원으로 나눌 수 있는 초평면을 찾는 분류 기법이다.

  - **Kernel Trick**:  저차원 공간에서 선형 분리가 되지 않는 데이터들을 고차원 공간상에 매핑 시켜서 선형 분리되도록 하는 기법 (ex. XOR 분류문제)

  - 이때 사용하는 함수가 **Kernel Function**이다. 주로 사용하는 함수로는

    (가우시안) RBF (Radial Basis Function) 커널, 다항식 커널, tanh, sigmoid, 유사도 특성 등이 있다



- 장점

  1. Noise Data 영향을 크게 받지 않으며, 과적합화 (오버피팅)가 잘 되지 않음

  2. (신경망 기법에 비해) 분류 문제에 대한 성능이 높음

  3. 분류 경계가 복잡한 비선형 문제의 경우, 타 기법 대비 성능 우수

  4. 분류문제, 예측 문제에 모두 사용 가능

     

- 단점
  1. 모델 구축 시 커널 함수 및 매개 변수 등에 대한 반복적인 조합 테스트 필요 
  2. 입력 데이터 양, 변수가 많으면 오랜 시간의 훈련 필요
  3.  다른 기법과 비교하여 난해한 배경 이론 및 알고리즘 구현
  4. 결과에 대한 해석이나 이유 설명 등이 어려움





#### - 코딩 실습 -

- Data set: Iris 붓꽃 데이터

```python
import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Data 불러와서 결측치, 중복치 전처리
iris = datasets.load_iris()
iris.keys() #dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
iris["feature_names"]
# ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

df = pd.DataFrame(iris['data'], columns=iris['feature_names'])
df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

# Target 열 추가
df['Target'] = iris['target']
print('데이터셋의 크기: ', df.shape) # 데이터셋의 크기:  (150, 5)

df.isnull().sum()
df.duplicated().sum() # 중복 데이터 확인
df.loc[df.duplicated(), :] # 중복 데이터 있으면 슬라이싱
df.loc[(df.sepal_length==5.8)&(df.petal_width==1.9), :] # 중복 데이터 모두 출력
df = df.drop_duplicates() # 중복 데이터 제거: 자주 사용
df.loc[(df.sepal_length==5.8)&(df.petal_width==1.9), :] # 제거 되었는지 확인

# Data Set 분할
x_data = df.loc[:, "sepal_length":"petal_width"]
y_data = df.loc[:, "Target"]
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)

#서포트벡터머신 객체 만들고, fit, predict
svc = SVC()
svc.fit(x_train, y_train)
y_predict = svc.predict(x_train)
y_test_preict = svc.predict(x_test)
accuracy_score(y_train, y_predict) # 0.9747899159663865
accuracy_score(y_test, y_test_predict) # 0.9666666666666667



# GridSearchCV 활용하여 최적의 하이퍼파라미터 구하기
# C: SVM 모델이 분류할 때 어느정도 오차를 허용할 건지 판단=> 적당한 값을 찾는게 베스트!
# 마진 내부에 오류를 얼마 만큼 허용할 것인가?
# 하드 마진: c 커질 수록 오류를 허용하지 않는다 (성능 연관)
# 소프트 마진: c 작아질 수록 오류를 많이 허용 (일반화에 좋음)
# uniform: 각 이웃이 동일한 가중치, distance: 가까운 이웃이 멀리 있는 이웃보다 큰 가중치
# degree: 다항식의 차수

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.01,0.1,1, 10], 
               'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 
               'degree': [1,2,3,]}

gs = GridSearchCV(
    estimator = SVC(),
    param_grid = param_grid,
    n_jobs=2,
    verbose = True 
)

gs.fit(x_data, y_data)
# Fitting 5 folds for each of 48 candidates, totalling 240 fits
# GridSearchCV(estimator=SVC(), n_jobs=2,
#             param_grid={'C': [0.01, 0.1, 1, 10], 'degree': [1, 2, 3],
#                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},
#             verbose=True)

gs.best_params_ # {'C': 1, 'degree': 2, 'kernel': 'poly'}
gs.best_score_ # 0.9866666666666667
```





### SVM 회귀

- SVM은 선형, 비선형 분류 뿐만 아니라 선형, 비선형 회귀에도 사용할 수 있다.
- 마진을 결정하고 최대한 Dataset이 많이 들어가게 만들어 준다.
- 제한된 마진 오류 안에서 도로 안에 가능한 한 많은 샘플이 들어가도록 학습한다.