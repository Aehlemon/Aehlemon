### 🔎 Ensemble Analysis (앙상블 분석)

- 개념: 여러 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높이는 기법

  - 새로운 자료에 대한 분류 예측 값을 **가중 투표**하여 분류함

  - 일반적으로 어떤 데이터 값을 예측할 때 하나의 모델을 사용하는 것보다 여러 개의 모델을

    조화롭게 학습 시켜 그 모델들의 예측결과를 이용하여 더 정확한 예측값을 구할 수 있음

    - 여러 개의 Decision Tree 를 결합하여 1개의 결정 트리보다 더 좋은 성능을 내는 방법을 사용

    - **여러 개의 약 분류기 (Weak Classifier)를 결합하여 강 분류기(Strong Classifier)를 만듦**

      

  - **Bagging (배깅)**: Bootstrap Aggregation을 의미, 샘플을 여러 번 뽑아 (Bootstrap) 각 모델을 학습시켜

    결과물을 집계(Aggregation) 한다. ("일반적인 모델을 만드는데 집중")

    - 병렬로 학습 (Parallel Training)

    - <u>대표적인 알고리즘 Ex: **랜덤 포레스트 (Random Forest)**</u>

      

  - **Boosting (부스팅)**:  ("맞히기 어려운 문제에 집중", 순차적으로 학습시킴_Sequential Training)

  - 가중치를 활용하여 약 분류기 → 강 분류기로 만드는 방법

    - 원리: 처음으로 만든 모델 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 

      다음 모델에 영향을 준다. 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복함

    - 일반적으로 배깅에 비해 부스팅의 에러가 적고 성능이 좋다. 

      반면, 속도가 느리고  Over fitting 될 가능성이 높으며, 이상값 (Outlier)에 취약

    - 배깅과 부스팅 모두 복원 랜덤 샘플링 (sampling with replacement), 차이점은 <u>부스팅은 가중치 부여</u>

    - 대표적인 알고리즘 Ex: **AdaBoost, GBM**(Gradient Boosting Machine),  **XGBoost, Arc-x4**

      - AdaBoost: 약 분류학습기의 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 알고리즘

      - GBM: 경사하강법을 통해 가중치 업데이트를 수행하여 최적화된 결과를 얻는 알고리즘

        → 예측 성능이 높지만, Greedy Algorithm으로 과적합이 빨리 되거나 시간이 오래걸리는 단점



  **Stacking** : 서로 다른 모델들을 조합하여 최고의 성능을 내는 모델 생성 방법
  => stacking에 대한 자세한 내용을 더 공부해보고 정리해봐야겠다.
  

