### ğŸ” ë”¥ëŸ¬ë‹ ê²½ì‚¬í•˜ê°•ë²•



#### ê²½ì‚¬ë²•(Gradient Method)ì˜ ê°œë…

- ì‹ ê²½ë§ ê¸°ê³„ í•™ìŠµ ì‹œ ìµœì ì˜ ë§¤ê°œ ë³€ìˆ˜ ê°’ì„ ì°¾ëŠ”ë°, ìµœì ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì°¾ê¸° ìœ„í•´ ì†ì‹¤í•¨ìˆ˜ê°€ ìµœì†Ÿê°’ì´ ë˜ëŠ” ì§€ì ì„ ì°¾ì•„ì•¼í•¨

- ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•´ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ ë˜ëŠ” ê°€ëŠ¥í•œ ì‘ì€ ê°’ì„ ì°¾ìœ¼ë ¤ëŠ” ê²ƒì´ `ê²½ì‚¬ë²•`

- ìµœì†Ÿê°’, ìµœëŒ“ê°’ì„ ì°¾ëŠëƒì— ë”°ë¼ ê²½ì‚¬ í•˜ê°•ë²•, ê²½ì‚¬ ìƒìŠ¹ë²•ìœ¼ë¡œ ë‚˜ë‰˜ëŠ”ë° ê²½ì‚¬ í•˜ê°•ë²•ì„ ë” ë§ì´ ì”€

- ê²½ì‚¬ë²• ìˆ˜ì‹

  

<img src="../Python/ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹/img/e4.7.png" alt="e4.7" style="zoom:50%;" />

1) **Î·** (ì—íƒ€): **Learning rate,  í•™ìŠµë¥ **
   - í•œë²ˆì˜ í•™ìŠµìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ í•™ìŠµí•´ì•¼í•  ì§€ (ë§¤ê°œ ë³€ìˆ˜ ê°’ì„ ì–¼ë§ˆë‚˜ ê°±ì‹ í•˜ëŠ” ì§€ ì˜í–¥ì„ ë¯¸ì¹¨)
   - í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyper parameter): í•™ìŠµë¥  ê°™ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì¼ì»«ëŠ” ë§, ê°€ì¤‘ì¹˜ & í¸í–¥ê³¼ëŠ” ì„±ì§ˆì´ ë‹¤ë¥¸ ë§¤ê°œ ë³€ìˆ˜
   - ê°€ì¤‘ì¹˜, í¸í–¥ â†’ í›ˆë ¨ ë°ì´í„°ì™€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ìë™ì ìœ¼ë¡œ í•™ìŠµ, íšë“ ê°€ëŠ¥
   - í•™ìŠµë¥ : ì‚¬ëŒì´ ì§ì ‘ ì„¤ì •í•´ì•¼ í•¨
2) ì‹ì—ì„œëŠ” ë³€ìˆ˜ 2ê°œì¸ ê²½ìš°ë¡œ ë‚˜íƒ€ëƒˆì§€ë§Œ ë³€ìˆ˜ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë„ ê°™ì€ ì‹ (ê° ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„) ê°’ìœ¼ë¡œ ê°±ì‹ í•˜ê²Œ ë¨



### ê²½ì‚¬ë²•ì˜ ì¢…ë¥˜

> - [ì°¸ê³ ìë£Œ 1](https://acdongpgm.tistory.com/202):    Full batch, Mini batch, SGD ì„¤ëª…
>
> - [ì°¸ê³ ìë£Œ 2](https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam) : Optimizer ì¢…ë¥˜ì™€ íŠ¹ì„± 



- Gradient Descent (Full batch, GD):  Batch size =m (ì „ì²´), 1ê°œì˜ ë°°ì¹˜ë§Œ ì¡´ì¬í•˜ê³  1 ë°°ì¹˜ê°€ í•™ìŠµë˜ì–´ ì—…ë°ì´íŠ¸ ë¨ 
  - ëª¨ë“  ìë£Œë¥¼ ë‹¤ ê²€í† í•´ì„œ ìµœì í™”ë˜ëŠ” ê¸°ìš¸ê¸° ë°©í–¥ì„ ì°¾ìŒ
- Mini Batch Gradient Descent: í›ˆë ¨ë°ì´í„°ì—ì„œ 2ì§„ë²•ì  ì ‘ê·¼ìœ¼ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì„ íƒí•˜ì—¬ cost function ê³„ì‚° ë° ê²½ì‚¬ í•˜ê°• ì ìš©
  - SGDì˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ë©´ì„œë„ ì „ì²´ ë°°ì¹˜ë³´ë‹¤ëŠ” ë” ë¹ ë¥´ê²Œ ìµœì ì ì„ êµ¬í•¨
- Stochastic Gradient Descent (SGD) í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•: í™•ë¥ ì ìœ¼ë¡œ ë¬´ì‘ìœ„ë¡œ ê³¨ë¼ë‚¸ ë¯¸ë‹ˆ ë°°ì¹˜ í™œìš©
  - ì˜ˆì‹œ: 100ê°œ ì¤‘ 1ê°œë¥¼ ë½‘ì•„ì„œ í•™ìŠµí•˜ê³  ê°±ì‹ , 1ê°œ ë½‘ì•„ì„œ ê°±ì‹  (100íšŒ ë°˜ë³µ)
- Momentum: ê²½ì‚¬ì˜ ë°©í–¥ì´ ìœ„ì—ì„œ ì•„ë˜ë¡œ í˜ëŸ¬ê°€ëŠ” ëª¨ìŠµì„ ìš´ë™ìœ¼ë¡œ í‘œí˜„
  - ë¬¸ì œì : í•™ìŠµ ì†ë„ê°€ ëŠë¦¬ë©°, í‰íƒ„í•œ ì§€ì ì—ì„œ ë”ì´ìƒ í•™ìŠµ ë¶ˆê°€
- Adagrad: ê°€ì¤‘ì¹˜ì˜ ì—…ë°ì´íŠ¸ íšŸìˆ˜ì— ë”°ë¼ step sizeë¥¼ ë‹¤ë¥´ê²Œ ì¡°ì ˆ
  - ë³€í™”í•˜ì§€ ì•ŠëŠ” ê°€ì¤‘ì¹˜ë“¤ì€ step size í¬ê²Œ, ë³€í™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë“¤ì€ step size ì‘ê²Œ
  - dL/dW: Wì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°
  - h: ì†ì‹¤í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì œê³±í•˜ì—¬ ê³„ì† ë”í•´ì¤€ë‹¤.



### ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµ êµ¬í˜„

- í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ êº¼ë‚´ê³ , ë¯¸ë‹ˆ ë°°ì¹˜ì— ëŒ€í•´ ê²½ì‚¬ë²•ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 



```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize = True, one_hot_label = True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000  # ë°˜ë³µ íšŸìˆ˜ë¥¼ ì ì ˆíˆ ì„¤ì •í•œë‹¤.
train_size = x_train.shape[0]
batch_size = 100   # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
learning_rate = 0.1

# ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°ë¥¼ 100ìœ¼ë¡œ ì„¤ì •. í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ìˆ˜í–‰í•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ .
# ê²½ì‚¬ë²•ì— ì˜í•œ ê°±ì‹  íšŸìˆ˜ëŠ” 10,000ë²ˆìœ¼ë¡œ ì„¤ì •, ê°±ì‹ í•  ë•Œë§ˆë‹¤ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ê·¸ ê°’ì„ ë°°ì—´ì— ì¶”ê°€

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ íšë“
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚°
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

 # í‘œë¡œ ì†ì‹¤í•¨ìˆ˜ í™•ì¸
import matplotlib.pyplot as plt

f, (ax1, ax2) = plt.subplots(2, 1)
x = np.array(range(iters_num))
ax1.plot(x, train_loss_list, label='loss')
ax1.set_xlabel("iteration")
ax1.set_ylabel("loss")
ax1.set_ylim(0, 3.0)
ax2.plot(x[:1000], train_loss_list[:1000], label='loss')
ax2.set_xlabel("iteration")
ax2.set_ylabel("loss")
ax2.set_ylim(0, 3.0)

# ì‹œí—˜ ë°ì´í„° í‰ê°€ ê·¸ë˜í”„
# ê·¸ë¦¼ 4-12 í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ ì¶”ì´
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```



- í•™ìŠµ íšŒìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì´ ì¤„ì–´ë“¬ â†’ í•™ìŠµì´ ì˜ ë˜ê³  ìˆë‹¤ëŠ” ëœ»
- ë°ì´í„°ë¥¼ ë°˜ë³µí•´ì„œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ìµœì  ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì„œì„œíˆ ë‹¤ê°€ì„œê³  ìˆëŠ” ê²ƒì„ í™•ì¸ ê°€ëŠ¥



<img src="Day_0322.assets/image-20220322233040752.png" alt="image-20220322233040752" style="zoom:80%;" />



**ì‹œí—˜ ë°ì´í„°ë¡œ í‰ê°€í•˜ê¸°**

- í›ˆë ¨ ë°ì´í„° ì™¸ì˜ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”. 'ì˜¤ë²„í”¼íŒ…'ì„ ì¼ìœ¼í‚¤ì§€ ì•ŠëŠ”ì§€ í™•ì¸.
  - **ì˜¤ë²„í”¼íŒ…**: í›ˆë ¨ ë°ì´í„°ì— í¬í•¨ëœ ì´ë¯¸ì§€ë§Œ ì œëŒ€ë¡œ êµ¬ë¶„. ê·¸ë ‡ì§€ ì•Šì€ ì´ë¯¸ì§€ëŠ” ì‹ë³„í•  ìˆ˜ ì—†ë‹¤ëŠ” ì˜ë¯¸

- ë²”ìš© ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ í•™ìŠµ ë„ì¤‘ ì •ê¸°ì ìœ¼ë¡œ í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì •í™•ë„ë¥¼ ê¸°ë¡
  - 1ì—í­ë³„ë¡œ í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ê¸°ë¡
  - ì—í­(epoch): 1ì—í­ì€ í•™ìŠµì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ë‘ ì†Œì§„í–ˆì„ ë•Œì˜ íšŒìˆ˜ì— í•´ë‹¹
  - í›ˆë ¨ ë°ì´í„° 10,000ê°œë¥¼ 100ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜ë¡œ í•™ìŠµí•  ê²½ìš°, 100íšŒê°€ 1ì—í­





___



>  *"ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹"*  ì±…ì„ ì°¸ê³ í•˜ë©° ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.